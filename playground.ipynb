{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 1000: 0.058\n",
      "Average loss at step 2000: 0.030\n",
      "Average loss at step 3000: 0.020\n",
      "Average loss at step 4000: 0.015\n",
      "Average loss at step 5000: 0.012\n",
      "Average loss at step 6000: 0.010\n",
      "Accuracy at step 6250: 0.062\n",
      "Average loss at step 7000: 0.009\n",
      "Average loss at step 8000: 0.008\n",
      "Average loss at step 9000: 0.007\n",
      "Average loss at step 10000: 0.006\n",
      "Average loss at step 11000: 0.005\n",
      "Average loss at step 12000: 0.005\n",
      "Average loss at step 13000: 0.005\n",
      "Average loss at step 14000: 0.004\n",
      "Average loss at step 15000: 0.004\n",
      "Average loss at step 16000: 0.004\n",
      "Average loss at step 17000: 0.004\n",
      "Average loss at step 18000: 0.003\n",
      "Accuracy at step 18750: 0.062\n",
      "Average loss at step 19000: 0.003\n",
      "Average loss at step 20000: 0.003\n",
      "Average loss at step 21000: 0.003\n",
      "Average loss at step 22000: 0.003\n",
      "Average loss at step 23000: 0.003\n",
      "Average loss at step 24000: 0.003\n",
      "Average loss at step 25000: 0.002\n",
      "Average loss at step 26000: 0.002\n",
      "Average loss at step 27000: 0.002\n",
      "Average loss at step 28000: 0.002\n",
      "Average loss at step 29000: 0.002\n",
      "Average loss at step 30000: 0.002\n",
      "Average loss at step 31000: 0.002\n",
      "Accuracy at step 31250: 0.062\n",
      "Average loss at step 32000: 0.002\n",
      "Average loss at step 33000: 0.002\n",
      "Average loss at step 34000: 0.002\n",
      "Average loss at step 35000: 0.002\n",
      "Average loss at step 36000: 0.002\n",
      "Average loss at step 37000: 0.002\n",
      "Average loss at step 38000: 0.002\n",
      "Average loss at step 39000: 0.002\n",
      "Average loss at step 40000: 0.002\n",
      "Average loss at step 41000: 0.001\n",
      "Average loss at step 42000: 0.001\n",
      "Average loss at step 43000: 0.001\n",
      "Accuracy at step 43750: 0.062\n",
      "Average loss at step 44000: 0.001\n",
      "Average loss at step 45000: 0.001\n",
      "Average loss at step 46000: 0.001\n",
      "Average loss at step 47000: 0.001\n",
      "Average loss at step 48000: 0.001\n",
      "Average loss at step 49000: 0.001\n",
      "Average loss at step 50000: 0.001\n",
      "Average loss at step 51000: 0.001\n",
      "Average loss at step 52000: 0.001\n",
      "Average loss at step 53000: 0.001\n",
      "Average loss at step 54000: 0.001\n",
      "Average loss at step 55000: 0.001\n",
      "Average loss at step 56000: 0.001\n",
      "Accuracy at step 56250: 0.062\n",
      "Average loss at step 57000: 0.001\n",
      "Average loss at step 58000: 0.001\n",
      "Average loss at step 59000: 0.001\n",
      "Average loss at step 60000: 0.001\n",
      "Average loss at step 61000: 0.001\n",
      "Average loss at step 62000: 0.001\n",
      "Finished 62500 steps with avg. loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "class GRUCell(object):\n",
    "    def __init__(self, state_size, activation=tf.sigmoid):\n",
    "        self._state_size = state_size\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "    \n",
    "#     def zero_state(self, batch_size):\n",
    "#         return \n",
    "        \n",
    "    def __call__(self, s, scope=None):\n",
    "        with tf.name_scope('GRUCell'):\n",
    "            \n",
    "            # trainable variables\n",
    "            with tf.variable_scope('weights', initializer=tf.contrib.layers.xavier_initializer()) as scope:\n",
    "                W_u = tf.get_variable('W_u', [self._state_size, self._state_size])\n",
    "                W_r = tf.get_variable('W_r', [self._state_size, self._state_size])\n",
    "                W   = tf.get_variable('W', [self._state_size, self._state_size])\n",
    "                b   = tf.get_variable('b', [self._state_size], initializer=tf.constant_initializer(1.0))\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "            # u = update, r = reset, c = candidates, h = new hidden states\n",
    "            u = self._activation(tf.matmul(s, W_u) + b)\n",
    "            r = self._activation(tf.matmul(s, W_r) + b)\n",
    "            c = tf.tanh(tf.matmul(tf.multiply(s, r), W) + b)\n",
    "            h = tf.multiply(s, u) + tf.multiply((1 - u), c)\n",
    "            return h, h\n",
    "\n",
    "        \n",
    "class ca_rnn(object):\n",
    "    def __init__(self,\n",
    "                 x,\n",
    "                 y=None,\n",
    "                 learning_rate=1e-4,\n",
    "                 state_size=5,\n",
    "                 batch_size=1):\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "        # Inference\n",
    "        with tf.name_scope('inference'):\n",
    "            width = 5\n",
    "            depth = 1\n",
    "            classes = 2\n",
    "            time_steps = 1\n",
    "\n",
    "            # RNN\n",
    "            with tf.name_scope('rnn'):\n",
    "                # GRU cell\n",
    "                cell = GRUCell(state_size)\n",
    "\n",
    "                # Just one time step\n",
    "                zero_state = tf.reshape(x, [batch_size, width])  # load all X on state zero\n",
    "#                 zero_state = x\n",
    "                output, state = cell(zero_state)\n",
    "\n",
    "            # Convolution\n",
    "            with tf.name_scope('convolution'):\n",
    "                output = tf.reshape(output, [batch_size, width, depth])\n",
    "                kernel = tf.constant([1, 1, 1], shape=[3, 1, 1], dtype=tf.float32, name='kernel')\n",
    "                conv = tf.nn.conv1d(output, kernel, stride=1, padding='SAME')\n",
    "\n",
    "            # FC1\n",
    "            with tf.name_scope('fc1'):\n",
    "                with tf.variable_scope('output'):\n",
    "                    W = tf.get_variable('W_out', [state_size, classes])\n",
    "                    b = tf.get_variable('b_out', [classes], initializer=tf.constant_initializer(1.0))\n",
    "\n",
    "                flatten = tf.reshape(conv, [batch_size, width])\n",
    "                fc1 = tf.nn.xw_plus_b(flatten, W, b, name='fc1')\n",
    "#                 fc1 = tf.reshape(fc1, [batch_size, classes, depth])\n",
    "\n",
    "            self._logits = fc1\n",
    "        \n",
    "        # loss function\n",
    "        with tf.name_scope('loss'):\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=self._logits, labels=y, name='cross_entropy')\n",
    "            self._loss = tf.reduce_mean(cross_entropy)\n",
    "            # add scalar histogram for loss\n",
    "            \n",
    "            self._optimizer = tf.train.AdamOptimizer(self._learning_rate).minimize(self._loss)\n",
    "        \n",
    "        # evaluation\n",
    "        with tf.name_scope('prediction'):\n",
    "            correct = tf.nn.in_top_k(self._logits, y, 1)\n",
    "            self._prediction = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    @property\n",
    "    def inference(self):\n",
    "        return self._logits\n",
    "    \n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "    \n",
    "    @property\n",
    "    def prediction(self):\n",
    "        return self._prediction\n",
    "\n",
    "\n",
    "class Dataset1d(object):\n",
    "    def __init__(self, x, y):\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        self._n_samples = x.shape[0]\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        \n",
    "    def next_batch(self, batch_size, shuffle=True):\n",
    "        start = self._index_in_epoch\n",
    "        \n",
    "        if self._epochs_completed == 0 and start == 0 and shuffle:\n",
    "            perm0 = np.arange(self._n_samples)\n",
    "            np.random.shuffle(perm0)\n",
    "            self._x = self._x[perm0]\n",
    "            self._y = self._y[perm0]\n",
    "            \n",
    "        if start + batch_size > self._n_samples:\n",
    "            # finished\n",
    "            self._epochs_completed += 1\n",
    "            rest_samples = self._n_samples - start\n",
    "            x_rest_part = self._x[start:self._n_samples]\n",
    "            y_rest_part = self._y[start:self._n_samples]\n",
    "            \n",
    "            if shuffle:\n",
    "                perm = np.arange(self._n_samples)\n",
    "                np.random.shuffle(perm)\n",
    "                self._x = self._x[perm]\n",
    "                self._y = self._y[perm]\n",
    "\n",
    "            # Start next epoch.\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_samples\n",
    "            end = self._index_in_epoch\n",
    "            x_new_part = self._x[start:end]\n",
    "            y_new_part = self._y[start:end]\n",
    "            \n",
    "            return np.concatenate((x_rest_part, x_new_part), axis=0) , np.concatenate((y_rest_part, y_new_part), axis=0)\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            \n",
    "            return self._x[start:end], self._y[start:end]\n",
    "    \n",
    "    @property\n",
    "    def x(self):\n",
    "        return self._x\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "    \n",
    "    @property\n",
    "    def n_samples(self):\n",
    "        return self._n_samples\n",
    "    \n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "\n",
    "def build_1d_dataset(\n",
    "    width=8,\n",
    "    depth=1,\n",
    "    n_samples=100,\n",
    "    k_value=2,\n",
    "    train_split=0.8,\n",
    "    valid_split=0.5,\n",
    "    verbose=False):\n",
    "    \n",
    "    x = np.random.randint(0, k_value, size=[n_samples, width, depth])\n",
    "    y = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    # samples, [width, depth]\n",
    "    for i, board in enumerate(x):\n",
    "        # count connection length\n",
    "        connection_length = 0\n",
    "        # width, depth\n",
    "        for j, grid in enumerate(board):\n",
    "            if grid == [1]:\n",
    "                connection_length += 1 \n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        if connection_length == width:\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "#         y[i] = connection_length\n",
    "\n",
    "    dataset = namedtuple('Dataset', ['train', 'valid', 'test'])\n",
    "    \n",
    "    # Split dataset\n",
    "    n_train = int(n_samples * train_split)\n",
    "    n_valid = int((n_samples - n_train) * valid_split)\n",
    "    \n",
    "    dataset.train = Dataset1d(x[:n_train], y[:n_train])\n",
    "    dataset.valid = Dataset1d(x[:n_valid], y[:n_valid])\n",
    "    dataset.test = Dataset1d(x[:n_valid], y[:n_valid])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train_network(model, \n",
    "                  dataset, \n",
    "                  batch_size=1, \n",
    "                  n_epochs=1):\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        writer = tf.summary.FileWriter('./graphs/run1', sess.graph)\n",
    "\n",
    "        average_loss = .0\n",
    "        n_batches = int(dataset.train.n_samples / batch_size)\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch in range(n_batches):\n",
    "                x, y = dataset.train.next_batch(batch_size)\n",
    "\n",
    "                # train\n",
    "                loss_batch, _, global_step = sess.run([model.loss, model.optimizer, increment_step], \n",
    "                                         feed_dict={x_pl: x, y_pl: y} )\n",
    "                average_loss += loss_batch\n",
    "\n",
    "                if (global_step) % 1000 == 0:\n",
    "                    print('Average loss at step {}: {:5.3f}'.format(global_step,\n",
    "                                                                    average_loss / global_step))\n",
    "            # validate now and again\n",
    "            if epoch % 2 == 0:\n",
    "                total_correct_pred = 0\n",
    "                for batch in range(n_batches):\n",
    "                    x_valid, y_valid = dataset.valid.next_batch(batch_size)\n",
    "                    valid_loss, prediction = sess.run([model.loss, model.prediction], \n",
    "                                         feed_dict={x_pl: x_valid, y_pl: y_valid} )\n",
    "                    total_correct_pred += prediction\n",
    "                print('Accuracy at step {}: {:5.3f}'.format(global_step,\n",
    "                                                                    total_correct_pred / dataset.valid.n_samples))\n",
    "                \n",
    "        writer.close()\n",
    "    print('Finished {} steps with avg. loss: {:5.3f}'.format(global_step, average_loss / global_step))\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholder [batch, width, depth]\n",
    "with tf.name_scope('placeholders'):\n",
    "    x_pl = tf.placeholder(tf.float32, shape=[None, 5, 1], name='x')\n",
    "    y_pl = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "increment_step = global_step.assign_add(1)\n",
    "\n",
    "dataset = build_1d_dataset(width=5, n_samples=1000000)\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "model = ca_rnn(x_pl, y_pl, batch_size=batch_size, learning_rate=0.01)\n",
    "train_network(model, dataset, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2.]\n",
      "  [ 3.]\n",
      "  [ 2.]\n",
      "  [ 2.]\n",
      "  [ 1.]]\n",
      "\n",
      " [[ 2.]\n",
      "  [ 3.]\n",
      "  [ 3.]\n",
      "  [ 3.]\n",
      "  [ 2.]]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example\n",
    "1d convolution in tensorflow\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# filter [width, channel in, channel out]\n",
    "# U = tf.Variable(np.array([.5, .5, .5]).reshape(3, 1, 1), dtype=tf.float32)\n",
    "U = tf.Variable([1., 1., 1.], dtype=tf.float32)\n",
    "U = tf.reshape(U, [3, 1, 1])\n",
    "\n",
    "b = tf.Variable(np.array([0., 0., 0., 0., 0.]).reshape(5, 1), dtype=tf.float32)\n",
    "\n",
    "# input [batch, length, channels/dims]\n",
    "s = tf.constant(np.array([[1., 1., 1., 0., 1.], [1., 1., 1., 1., 1.]]).reshape(2, 5, 1), dtype=tf.float32)\n",
    "\n",
    "\n",
    "# x = tf.matmul(s, U)\n",
    "# tf.nn.convolution(s, [3, 1, 1, 1], padding='SAME', strides=[1, 1, 1, 1])\n",
    "x = tf.nn.conv1d(s, U, stride=1, padding='SAME') + b\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print(sess.run(x))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.99997318]\n",
      "  [ 2.017946  ]\n",
      "  [ 1.03593218]\n",
      "  [ 1.03593218]\n",
      "  [ 1.01795936]]\n",
      "\n",
      " [[ 1.99999869]\n",
      "  [ 2.99999809]\n",
      "  [ 2.00669098]\n",
      "  [ 2.00669098]\n",
      "  [ 1.00669158]]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example\n",
    "GRU cell single pass\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# input [batch, length]\n",
    "state_size = 5\n",
    "batch = 2\n",
    "width = 5\n",
    "depth = 1\n",
    "\n",
    "# placeholder [batch, width, depth]\n",
    "x = tf.constant([1, 1, 0, 0, 1, 1, 1, 1, 0, 1], shape=[batch, width, depth], dtype=tf.float32)\n",
    "\n",
    "# trainable variables\n",
    "W_u = tf.Variable(tf.ones([state_size, state_size]), dtype=tf.float32)\n",
    "W_r = tf.Variable(tf.ones([state_size, state_size]), dtype=tf.float32)\n",
    "W = tf.Variable(tf.ones([state_size, state_size]), dtype=tf.float32)\n",
    "b = tf.constant([1, 1, 1, 1, 1], shape=[1, 5], dtype=tf.float32)\n",
    "\n",
    "# GRU\n",
    "s = tf.reshape(x, [batch, width])\n",
    "activation = tf.sigmoid\n",
    "\n",
    "u = activation(tf.matmul(s, W_u) + b)\n",
    "r = activation(tf.matmul(s, W_r) + b)\n",
    "c = tf.tanh(tf.matmul(tf.multiply(s, r), W) + b)\n",
    "h = tf.multiply(s, u) + tf.multiply((1 - u), c)\n",
    "\n",
    "# convolution\n",
    "output = tf.reshape(h, [batch, width, channels])\n",
    "kernel = tf.constant([1., 1., 1.], shape=[3, 1, 1], dtype=tf.float32)\n",
    "conv = tf.nn.conv1d(output, kernel, stride=1, padding='SAME')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(conv))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 36 42]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3, 1])\n",
    "a = tf.reshape(a, [2, 3])\n",
    "b = tf.constant([1, 2, 3, 4, 5, 6, 7, 8, 9], shape=[3, 3])\n",
    "c = tf.matmul(a, b)\n",
    "print(sess.run([a, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        outputs = []\n",
    "        for t, input_ in enumerate(inputs):\n",
    "            if t > 0: \n",
    "                scope.reuse_variables()\n",
    "                \n",
    "            output, state = cell(input_, state)\n",
    "            outputs.append(output)\n",
    "            \n",
    "#                     x = np.array([[1, 0, 0, 0, 1, 1, 1, 1, 0, 1]]).reshape([2, 5, 1])\n",
    "#                     y = np.array([0, 1]).reshape([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/fred/Developer/ca-rnn/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m(1035)\u001b[0;36m_do_call\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1033 \u001b[0;31m        \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1034 \u001b[0;31m          \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1035 \u001b[0;31m      \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1036 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1037 \u001b[0;31m  \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> cross_entropy.get_shape()\n",
      "*** NameError: name 'cross_entropy' is not defined\n",
      "ipdb> node_def\n",
      "name: \"loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\"\n",
      "op: \"SparseSoftmaxCrossEntropyWithLogits\"\n",
      "input: \"loss/SparseSoftmaxCrossEntropyWithLogits/Reshape\"\n",
      "input: \"loss/SparseSoftmaxCrossEntropyWithLogits/Reshape_1\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"Tlabels\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "\n",
      "ipdb> node_def.get_shape()\n",
      "*** AttributeError: 'NodeDef' object has no attribute 'get_shape'\n",
      "ipdb> op\n",
      "<tf.Operation 'loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' type=SparseSoftmaxCrossEntropyWithLogits>\n",
      "ipdb> op.get_shape()\n",
      "*** AttributeError: 'Operation' object has no attribute 'get_shape'\n",
      "ipdb> _logits\n",
      "*** NameError: name '_logits' is not defined\n",
      "ipdb> self._logits\n",
      "*** AttributeError: 'Session' object has no attribute '_logits'\n",
      "ipdb> model\n",
      "*** NameError: name 'model' is not defined\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
