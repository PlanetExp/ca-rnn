{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CA RNN\n",
    "Toy example in 1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 1000: 0.017\n",
      "Accuracy at step 1250: 0.125\n",
      "Average loss at step 2000: 0.008\n",
      "Average loss at step 3000: 0.006\n",
      "Accuracy at step 3750: 0.125\n",
      "Average loss at step 4000: 0.004\n",
      "Average loss at step 5000: 0.003\n",
      "Finished 5000 steps with avg. loss: 0.003\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "class GRUCell(object):\n",
    "    '''Standard GRU cell.\n",
    "    \n",
    "    Calling this cell on a RNN network returns a tuple of activations (h, h)\n",
    "    '''\n",
    "    def __init__(self, state_size, activation=tf.sigmoid):\n",
    "        self._state_size = state_size\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "    \n",
    "#     def zero_state(self, batch_size):\n",
    "#         return \n",
    "        \n",
    "    def __call__(self, s, scope=None):\n",
    "        with tf.name_scope('GRUCell'):\n",
    "            \n",
    "            # trainable variables\n",
    "            with tf.variable_scope('weights', initializer=tf.contrib.layers.xavier_initializer()) as scope:\n",
    "                W_u = tf.get_variable('W_u', [self._state_size, self._state_size])\n",
    "                W_r = tf.get_variable('W_r', [self._state_size, self._state_size])\n",
    "                W   = tf.get_variable('W', [self._state_size, self._state_size])\n",
    "                b   = tf.get_variable('b', [self._state_size], initializer=tf.constant_initializer(1.0))\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "            # u = update, r = reset, c = candidates, h = new hidden states\n",
    "            u = self._activation(tf.matmul(s, W_u) + b)\n",
    "            r = self._activation(tf.matmul(s, W_r) + b)\n",
    "            c = tf.tanh(tf.matmul(tf.multiply(s, r), W) + b)\n",
    "            h = tf.multiply(s, u) + tf.multiply((1 - u), c)\n",
    "            return h, h\n",
    "\n",
    "        \n",
    "class ca_rnn(object):\n",
    "    '''\n",
    "    A CA RNN neural network tensorflow model.\n",
    "    \n",
    "    Aims to generalise a CA algorithm from training data\n",
    "    \n",
    "    Properties:\n",
    "        inference:\n",
    "        loss:\n",
    "        optimizer:\n",
    "        prediction:\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 x,\n",
    "                 y=None,\n",
    "                 learning_rate=1e-4,\n",
    "                 state_size=5,\n",
    "                 batch_size=1):\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "        # Inference\n",
    "        with tf.name_scope('inference'):\n",
    "            width = 5\n",
    "            depth = 1\n",
    "            classes = 2\n",
    "            time_steps = 1\n",
    "\n",
    "            # RNN\n",
    "            with tf.name_scope('rnn'):\n",
    "                # GRU cell\n",
    "                cell = GRUCell(state_size)\n",
    "\n",
    "                # Just one time step\n",
    "                zero_state = tf.reshape(x, [batch_size, width])  # load all X on state zero\n",
    "#                 zero_state = x\n",
    "                output, state = cell(zero_state)\n",
    "\n",
    "            # Convolution\n",
    "            with tf.name_scope('convolution'):\n",
    "                output = tf.reshape(output, [batch_size, width, depth])\n",
    "                kernel = tf.Variable(tf.random_normal([3, 1, 1]), dtype=tf.float32, name='kernel')\n",
    "                conv = tf.nn.conv1d(output, kernel, stride=1, padding='SAME')\n",
    "\n",
    "            # FC1\n",
    "            with tf.name_scope('fc1'):\n",
    "                with tf.variable_scope('output'):\n",
    "                    W = tf.get_variable('W_out', [state_size, classes])\n",
    "                    b = tf.get_variable('b_out', [classes], initializer=tf.constant_initializer(1.0))\n",
    "\n",
    "                flatten = tf.reshape(conv, [batch_size, width])\n",
    "                fc1 = tf.nn.xw_plus_b(flatten, W, b, name='fc1')\n",
    "#                 fc1 = tf.reshape(fc1, [batch_size, classes, depth])\n",
    "\n",
    "            self._logits = fc1\n",
    "        \n",
    "        # loss function\n",
    "        with tf.name_scope('loss'):\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=self._logits, labels=y, name='cross_entropy')\n",
    "            self._loss = tf.reduce_mean(cross_entropy)\n",
    "            # add scalar histogram for loss\n",
    "            \n",
    "            self._optimizer = tf.train.AdamOptimizer(self._learning_rate).minimize(self._loss)\n",
    "        \n",
    "        # evaluation\n",
    "        with tf.name_scope('prediction'):\n",
    "            correct = tf.nn.in_top_k(self._logits, y, 1)\n",
    "            self._prediction = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    @property\n",
    "    def inference(self):\n",
    "        return self._logits\n",
    "    \n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "    \n",
    "    @property\n",
    "    def prediction(self):\n",
    "        return self._prediction\n",
    "\n",
    "\n",
    "def train_network(model, \n",
    "                  dataset, \n",
    "                  batch_size=1, \n",
    "                  n_epochs=1, \n",
    "                  tb_run=None):\n",
    "    '''\n",
    "    Function to train a neural network\n",
    "    '''\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Save data for tensorboard\n",
    "        writer = tf.summary.FileWriter('./graphs/run' + str(n_run), sess.graph)\n",
    "\n",
    "        average_loss = .0\n",
    "        n_batches = int(dataset.train.n_samples / batch_size)\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch in range(n_batches):\n",
    "                x, y = dataset.train.next_batch(batch_size)\n",
    "\n",
    "                # train\n",
    "                loss_batch, _, global_step = sess.run([model.loss, model.optimizer, increment_step], \n",
    "                                         feed_dict={x_pl: x, y_pl: y} )\n",
    "                average_loss += loss_batch\n",
    "\n",
    "                if (global_step) % 1000 == 0:\n",
    "                    print('Average loss at step {}: {:5.3f}'.format(global_step,\n",
    "                                                                    average_loss / global_step))\n",
    "            # validate now and again\n",
    "            if epoch % 2 == 0:\n",
    "                total_correct_pred = 0\n",
    "                for batch in range(n_batches):\n",
    "                    x_valid, y_valid = dataset.valid.next_batch(batch_size)\n",
    "                    valid_loss, prediction = sess.run([model.loss, model.prediction], \n",
    "                                         feed_dict={x_pl: x_valid, y_pl: y_valid} )\n",
    "                    total_correct_pred += prediction\n",
    "                print('Accuracy at step {}: {:5.3f}'.format(global_step,\n",
    "                                                                    total_correct_pred / dataset.valid.n_samples)) \n",
    "        writer.close()\n",
    "    print('Finished {} steps with avg. loss: {:5.3f}'.format(global_step, average_loss / global_step))\n",
    "\n",
    "    \n",
    "# Reset tensorflow variable names\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders [batch, width, depth]\n",
    "with tf.name_scope('placeholders'):\n",
    "    x_pl = tf.placeholder(tf.float32, shape=[None, 5, 1], name='x')\n",
    "    y_pl = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "# Tensorboard stats\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "increment_step = global_step.assign_add(1)\n",
    "n_run = 1\n",
    "\n",
    "# Dataset\n",
    "dataset = utils.build_1d_dataset(width=5, n_samples=100000)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "epochs = 4\n",
    "\n",
    "# Construct experiment\n",
    "model = ca_rnn(x_pl, y_pl, batch_size=batch_size, learning_rate=learning_rate)\n",
    "train_network(model, dataset, batch_size, epochs, n_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
